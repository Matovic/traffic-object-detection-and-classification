{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spracovanie videa použitím OpenCV YOLOv3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.6.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Any\n",
    "\n",
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cap_out(video_path:str, out_root:str='..', start_idx:int=15) -> Tuple[cv2.VideoCapture,\n",
    "                                                                              cv2.VideoWriter]:\n",
    "    \"\"\"\n",
    "    Read video capture and make video writer.\n",
    "    :param video_path:  path of the input \n",
    "    :param out_root:    path of the output folder\n",
    "    :param start_idx:   index for the name of the output video \n",
    "    returns: cv2.VideoCapture, cv2.VideoWriter \n",
    "    \"\"\"\n",
    "    # load video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # convert the resolutions from float to integer.\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "\n",
    "    # make video writer\n",
    "    out = cv2.VideoWriter(out_root + video_path[start_idx:-4] + '.avi', cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))\n",
    "    return cap, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060 Laptop GPU'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "torch.__version__\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.6.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.dnn.DNN_BACKEND_CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.dnn.DNN_TARGET_CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 694, 44, 176], [1, 720, 48, 154], [207, 734, 48, 143], [207, 745, 47, 137]]\n",
      "[[0, 703, 44, 154], [3, 715, 44, 161], [206, 739, 49, 133], [1273, 745, 57, 117], [204, 747, 51, 136], [1272, 757, 66, 112]]\n",
      "[[0, 694, 41, 168], [3, 713, 41, 167], [1275, 743, 47, 117], [205, 750, 48, 134], [1277, 761, 53, 105]]\n",
      "[[0, 696, 39, 163], [3, 711, 37, 172], [1277, 746, 44, 116], [202, 746, 47, 137], [1279, 754, 47, 117]]\n",
      "[[1548, 708, 45, 106], [1574, 707, 48, 102], [0, 694, 37, 170], [1, 707, 37, 180], [1272, 743, 48, 116], [201, 748, 49, 134]]\n",
      "[[0, 693, 35, 171], [1, 712, 36, 169], [1273, 745, 50, 113], [192, 752, 53, 128], [201, 747, 48, 135]]\n",
      "[[1571, 710, 53, 100], [-1, 699, 33, 159], [1, 713, 34, 168], [1274, 746, 46, 117], [191, 753, 54, 128], [199, 748, 50, 135]]\n",
      "[[-2, 704, 32, 152], [0, 719, 33, 156], [1264, 746, 51, 120], [1273, 747, 45, 113]]\n",
      "[[-1, 705, 29, 153], [0, 721, 31, 153], [188, 748, 52, 114], [1262, 744, 54, 123], [1272, 747, 45, 116], [187, 753, 55, 126], [1259, 757, 56, 112]]\n",
      "[[-3, 711, 32, 142], [0, 720, 32, 153], [188, 742, 46, 124], [1259, 746, 58, 117], [1269, 747, 49, 112], [190, 742, 48, 145], [1255, 760, 62, 106]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(\"../yolov3.weights\", \"../yolov3.cfg\")\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "\n",
    "# Define the labels of the classes\n",
    "classes = []\n",
    "with open('../yolov3.txt', 'r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Define the minimum confidence threshold and the non-maximum suppression threshold\n",
    "conf_threshold = 0.5\n",
    "nms_threshold = 0.4\n",
    "\n",
    "# Define the colors for drawing the bounding boxes\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "# Define the pedestrian tracker\n",
    "tracker = cv2.TrackerCSRT_create()\n",
    "\n",
    "# Initialize the bounding box\n",
    "bbox = None\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture('../PIE_data/PIE_clips/set01/video_0001.mp4')\n",
    "# convert the resolutions from float to integer.\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "out = cv2.VideoWriter('../outputs/video_0001.mp4', cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))\n",
    "\n",
    "counter = 0\n",
    "while True:\n",
    "    # Read a frame from the video\n",
    "    ret, frame = cap.read()\n",
    "    #print('ret')\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    #frame = cv2.resize(frame, None, fx=0.2,fy=0.2) #(600, 400))\n",
    "\n",
    "    # Resize the frame to the input size of the YOLO network\n",
    "    height, width = frame.shape[:2]\n",
    "    inp_size = (416, 416)\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255, inp_size, swapRB=True, crop=False)\n",
    "    #print('blob')\n",
    "    # Pass the blob through the network\n",
    "    net.setInput(blob)\n",
    "    output_layers = net.getUnconnectedOutLayersNames()\n",
    "    layer_outputs = net.forward(output_layers)\n",
    "    #print('net')\n",
    "\n",
    "    # Decode the output of the network\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "    #print('boxes')\n",
    "    for output in layer_outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > conf_threshold and class_id == 0:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # Apply non-maximum suppression to remove overlapping bounding boxes\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "    for i in indices:\n",
    "        #print(i)\n",
    "        #i = i[0]\n",
    "        #print(i)\n",
    "        x, y, w, h = boxes[i]\n",
    "        label = f\"{classes[class_ids[i]]}: {confidences[i]:.2f}\"\n",
    "        #color = colors[class_ids[i]]\n",
    "        #print()\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, label, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "        # Update the bounding box for the tracker\n",
    "        if bbox is None:\n",
    "            bbox = (x, y, w, h)\n",
    "            #tracker.init(frame, bbox)\n",
    "        #else:\n",
    "            #success, bbox = tracker.update(frame)\n",
    "            #if success:\n",
    "            #    x, y, w, h = [int(v) for v in bbox]\n",
    "            #    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            #else:\n",
    "            #    bbox = None\n",
    "    #print('here')\n",
    "    # Display the resulting frame\n",
    "    print(boxes)\n",
    "    frame = cv2.resize(frame, (600, 400))\n",
    "    cv2.imshow(\"Pedestrian detection\", frame)\n",
    "    cv2.waitKey(0)\n",
    "    # Press 'q' to quit\n",
    "    #if cv2.waitKey(1) == ord('q'):\n",
    "    #    break\n",
    "    #out.write(frame)\n",
    "    counter += 1\n",
    "    if counter % 10 == 0:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
