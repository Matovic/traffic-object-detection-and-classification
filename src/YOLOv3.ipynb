{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "config1 = [\n",
    "    (32, 3, 1),\n",
    "    (64, 3, 2),\n",
    "    [\"B\", 1],\n",
    "    (128, 3, 2),\n",
    "    [\"B\", 2],\n",
    "    (256, 3, 2),\n",
    "    [\"B\", 8],\n",
    "    # first route from the end of the previous block\n",
    "    (512, 3, 2),\n",
    "    [\"B\", 8],\n",
    "    # second route from the end of the previous block\n",
    "    (1024, 3, 2),\n",
    "    [\"B\", 4],\n",
    "    # until here is YOLO-53\n",
    "    (512, 1, 1),\n",
    "    (1024, 3, 1),\n",
    "    \"S\",\n",
    "    (256, 1, 1),\n",
    "    \"U\",\n",
    "    (256, 1, 1),\n",
    "    (512, 3, 1),\n",
    "    \"S\",\n",
    "    (128, 1, 1),\n",
    "    \"U\",\n",
    "    (128, 1, 1),\n",
    "    (256, 3, 1),\n",
    "    \"S\",\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.1)\n",
    "        self.use_bn_act = bn_act\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_bn_act:\n",
    "            return self.leaky(self.bn(self.conv(x)))\n",
    "        else:\n",
    "            return self.conv(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_residual=True, num_repeats=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for repeat in range(num_repeats):\n",
    "            self.layers += [\n",
    "                nn.Sequential(\n",
    "                    CNNBlock(channels, channels // 2, kernel_size=1),\n",
    "                    CNNBlock(channels // 2, channels, kernel_size=3, padding=1),\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        self.use_residual = use_residual\n",
    "        self.num_repeats = num_repeats\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x) + self.use_residual * x\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class ScalePrediction(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes): #anchors_per_scale\n",
    "        super(ScalePrediction, self).__init__()\n",
    "        self.pred = nn.Sequential(\n",
    "            CNNBlock(in_channels, 2*in_channels, kernel_size=3, padding=1),\n",
    "            CNNBlock(2*in_channels, (num_classes + 5) * 3, bn_act=False, kernel_size=1),\n",
    "        )\n",
    "        self.num_classes = num_classes\n",
    "        self.anchors_per_scale = 3 #anchors_per_scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (\n",
    "            self.pred(x)\n",
    "                .reshape(x.shape[0], self.anchors_per_scale, self.num_classes + 5, x.shape[2], x.shape[3])\n",
    "                .permute(0, 1, 3, 4, 2)\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(32, 3, 1), (64, 3, 2), ['B', 1], (128, 3, 2), ['B', 2], (256, 3, 2), ['B', 8], (512, 3, 2), ['B', 8], (1024, 3, 2), ['B', 4], (512, 1, 1), (1024, 3, 1), 'S', (256, 1, 1), 'U', (256, 1, 1), (512, 3, 1), 'S', (128, 1, 1), 'U', (128, 1, 1), (256, 3, 1), 'S']\n"
     ]
    }
   ],
   "source": [
    "print(config1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class YOLOv3(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=80):\n",
    "        super(YOLOv3, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.layers = self._create_conv_layers()\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        route_connections = []\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ScalePrediction):\n",
    "                outputs.append(layer(x))\n",
    "                continue\n",
    "\n",
    "            x = layer(x)\n",
    "\n",
    "            if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n",
    "                route_connections.append(x)\n",
    "\n",
    "            elif isinstance(layer, nn.Upsample):\n",
    "                x = torch.cat([x, route_connections[-1]], dim=1)\n",
    "                route_connections.pop()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def _create_conv_layers(self):\n",
    "        layers = nn.ModuleList()\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for module in config1:\n",
    "            if isinstance(module, tuple):\n",
    "                out_channels, kernel_size, stride = module\n",
    "                layers.append(\n",
    "                    CNNBlock(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=stride,\n",
    "                        padding=1 if kernel_size == 3 else 0,\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "\n",
    "            elif isinstance(module, list):\n",
    "                num_repeats = module[1]\n",
    "                layers.append(\n",
    "                    ResidualBlock(\n",
    "                        in_channels,\n",
    "                        num_repeats=num_repeats,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            elif isinstance(module, str):\n",
    "                if module == \"S\":\n",
    "                    layers += [\n",
    "                        ResidualBlock(in_channels, use_residual=False, num_repeats=1),\n",
    "                        CNNBlock(in_channels, in_channels // 2, kernel_size=1),\n",
    "                        ScalePrediction(in_channels // 2, num_classes=self.num_classes)\n",
    "                    ]\n",
    "                    in_channels = in_channels // 2\n",
    "\n",
    "                elif module == \"U\":\n",
    "                    layers.append(\n",
    "                        nn.Upsample(scale_factor=2),\n",
    "                    )\n",
    "                    in_channels = in_channels * 3\n",
    "\n",
    "        return layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def test():\n",
    "    num_classes = 20\n",
    "    img_size = 416\n",
    "    model = YOLOv3(num_classes=num_classes)\n",
    "\n",
    "    x = torch.randn((2, 3, img_size, img_size))\n",
    "    out = model(x)\n",
    "    assert out[0].shape == (2, 3, img_size//32, img_size//32, 5 + num_classes)\n",
    "    assert out[1].shape == (2, 3, img_size//16, img_size//16, 5 + num_classes)\n",
    "    assert out[2].shape == (2, 3, img_size//8, img_size//8, 5 + num_classes)\n",
    "\n",
    "test()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of Yolo Loss Function similar to the one in Yolov3 paper,\n",
    "the difference from what I can tell is I use CrossEntropy for the classes\n",
    "instead of BinaryCrossEntropy.\n",
    "\"\"\"\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import utils\n",
    "\n",
    "\n",
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.entropy = nn.CrossEntropyLoss()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Constants signifying how much to pay for each respective part of the loss\n",
    "        self.lambda_class = 1\n",
    "        self.lambda_noobj = 10\n",
    "        self.lambda_obj = 1\n",
    "        self.lambda_box = 10\n",
    "\n",
    "    def forward(self, predictions, target, anchors):\n",
    "        \"\"\"\n",
    "        :param predictions: output from model of shape: (batch size, anchors on scale , grid size, grid size, 5 + num classes)\n",
    "        :param target: targets on particular scale of shape: (batch size, anchors on scale, grid size, grid size, 6)\n",
    "        :param anchors: anchor boxes on the particular scale of shape (anchors on scale, 2)\n",
    "        :return: returns the loss on the particular scale\n",
    "        \"\"\"\n",
    "\n",
    "        # Check where obj and noobj (we ignore if target == -1)\n",
    "        # Here we check where in the label matrix there is an object or not\n",
    "        obj = target[..., 0] == 1  # in paper this is Iobj_i\n",
    "        noobj = target[..., 0] == 0  # in paper this is Inoobj_i\n",
    "\n",
    "        # ======================= #\n",
    "        #   FOR NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "        # The indexing noobj refers to the fact that we only apply the loss where there is no object\n",
    "        no_object_loss = self.bce(\n",
    "            (predictions[..., 0:1][noobj]), (target[..., 0:1][noobj]),\n",
    "        )\n",
    "\n",
    "        # ==================== #\n",
    "        #   FOR OBJECT LOSS    #\n",
    "        # ==================== #\n",
    "        # Here we compute the loss for the cells and anchor boxes that contain an object\n",
    "        # Reschape anchors to allow for broadcasting in multiplication below\n",
    "        anchors = anchors.reshape(1, 3, 1, 1, 2)\n",
    "        # Convert outputs from model to bounding boxes according to formulas in paper\n",
    "        box_preds = torch.cat([self.sigmoid(predictions[..., 1:3]), torch.exp(predictions[..., 3:5]) * anchors], dim=-1)\n",
    "        # Targets for the object prediction should be the iou of the predicted bounding box and the target bounding box\n",
    "        ious = utils.intersection_over_union(box_preds[obj], target[..., 1:5][obj]).detach()\n",
    "        # Only incur loss for the cells where there is an objects signified by indexing with obj\n",
    "        object_loss = self.bce((predictions[..., 0:1][obj]), (ious * target[..., 0:1][obj]))\n",
    "\n",
    "        # ======================== #\n",
    "        #   FOR BOX COORDINATES    #\n",
    "        # ======================== #\n",
    "        # apply sigmoid to x, y coordinates to convert to bounding boxes\n",
    "        predictions[..., 1:3] = self.sigmoid(predictions[..., 1:3])\n",
    "        # to improve gradient flow we convert targets' width and height to the same format as predictions\n",
    "        target[..., 3:5] = torch.log(\n",
    "            (1e-16 + target[..., 3:5] / anchors)\n",
    "        )\n",
    "        # compute mse loss for boxes\n",
    "        box_loss = self.mse(predictions[..., 1:5][obj], target[..., 1:5][obj])\n",
    "\n",
    "        # ================== #\n",
    "        #   FOR CLASS LOSS   #\n",
    "        # ================== #\n",
    "        # here we just apply cross entropy loss as is customary with classification problems\n",
    "        class_loss = self.entropy(\n",
    "            (predictions[..., 5:][obj]), (target[..., 5][obj].long()),\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            self.lambda_box * box_loss\n",
    "            + self.lambda_obj * object_loss\n",
    "            + self.lambda_noobj * no_object_loss\n",
    "            + self.lambda_class * class_loss\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#import config\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import (\n",
    "    cells_to_bboxes, # only for testing\n",
    "    iou_width_height as iou,\n",
    "    non_max_suppression as nms, # only for testing\n",
    "    plot_image #only for testing\n",
    ")\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True # na errory pri loadovani img"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a Pytorch dataset to load the Pascal VOC & MS COCO datasets\n",
    "\"\"\"\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_file,\n",
    "        img_dir,\n",
    "        label_dir,\n",
    "        anchors,\n",
    "        image_size=416,\n",
    "        S=[13, 26, 52], #grid sizes\n",
    "        C=20,\n",
    "        transform=None,\n",
    "    ):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "        self.S = S\n",
    "        self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])  # for all 3 scales\n",
    "        self.num_anchors = self.anchors.shape[0]\n",
    "        self.num_anchors_per_scale = self.num_anchors // 3\n",
    "        self.C = C\n",
    "        self.ignore_iou_thresh = 0.5\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n",
    "        bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist() # change to [x,y,w,h,class]\n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        # apply augmentations with albumentations\n",
    "        #nie je povinne\n",
    "        if self.transform:\n",
    "            augmentations = self.transform(image=image, bboxes=bboxes)\n",
    "            image = augmentations[\"image\"]\n",
    "            bboxes = augmentations[\"bboxes\"]\n",
    "\n",
    "        # Building the targets below:\n",
    "        # Below assumes 3 scale predictions (as paper) and same num of anchors per scale\n",
    "        targets = [torch.zeros((self.num_anchors // 3, S, S, 6)) for S in self.S] #S,S je grid size, 6-pravdepodobnost objekt,x,y,w,h,class\n",
    "        for box in bboxes:\n",
    "            iou_anchors = iou(torch.tensor(box[2:4]), self.anchors) #box[2:4]==height,width\n",
    "            anchor_indices = iou_anchors.argsort(descending=True, dim=0)\n",
    "            x, y, width, height, class_label = box\n",
    "            has_anchor = [False, False, False]  # each scale should have one anchor\n",
    "            for anchor_idx in anchor_indices:\n",
    "                scale_idx = anchor_idx // self.num_anchors_per_scale\n",
    "                anchor_on_scale = anchor_idx % self.num_anchors_per_scale\n",
    "                S = self.S[scale_idx]\n",
    "                i, j = int(S * y), int(S * x)  # which cell\n",
    "                anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n",
    "                if not anchor_taken and not has_anchor[scale_idx]:\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n",
    "                    x_cell, y_cell = S * x - j, S * y - i  # both between [0,1] #position in cell\n",
    "                    width_cell, height_cell = (\n",
    "                        width * S,\n",
    "                        height * S,\n",
    "                    )  # can be greater than 1 since it's relative to cell\n",
    "                    box_coordinates = torch.tensor(\n",
    "                        [x_cell, y_cell, width_cell, height_cell]\n",
    "                    )\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n",
    "                    has_anchor[scale_idx] = True\n",
    "\n",
    "                elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = -1  # ignore prediction\n",
    "\n",
    "        return image, tuple(targets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import config\n",
    "def test1():\n",
    "    anchors = config.ANCHORS\n",
    "\n",
    "    transform = config.train_transforms\n",
    "\n",
    "    dataset = YOLODataset(\n",
    "        config.DATASET+'/train.csv',\n",
    "        config.IMG_DIR,\n",
    "        config.LABEL_DIR,\n",
    "        S=[13, 26, 52],\n",
    "        anchors=anchors,\n",
    "        transform=transform,\n",
    "    )\n",
    "    print(dataset.annotations)\n",
    "    S = [13, 26, 52]\n",
    "    scaled_anchors = torch.tensor(anchors) / (\n",
    "        1 / torch.tensor(S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
    "    )\n",
    "    loader = DataLoader(dataset=dataset, batch_size=1, shuffle=True)\n",
    "    for x, y in loader:\n",
    "        boxes = []\n",
    "\n",
    "        for i in range(y[0].shape[1]):\n",
    "            anchor = scaled_anchors[i]\n",
    "            boxes += cells_to_bboxes(\n",
    "                y[i], is_preds=False, S=y[i].shape[2], anchors=anchor\n",
    "            )[0]\n",
    "        boxes = nms(boxes, iou_threshold=1, threshold=0.7, box_format=\"midpoint\")\n",
    "        print(boxes)\n",
    "        plot_image(x[0].permute(1, 2, 0).to(\"cpu\"), boxes)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     test1()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]D:\\skola\\ING\\LS2023\\NSIETE\\zadanie3\\venv\\lib\\site-packages\\torch\\amp\\autocast_mode.py:204: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.24s/it, loss=71.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.57s/it, loss=75.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.11s/it, loss=70.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.93s/it, loss=70.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.76s/it, loss=59.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.62s/it, loss=63.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.73s/it, loss=59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.70s/it, loss=58.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.79s/it, loss=62.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.81s/it, loss=58.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.71s/it, loss=54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "On Test loader:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class accuracy is: 5.555556%\n",
      "No obj accuracy is: 100.000000%\n",
      "Obj accuracy is: 0.000000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.65s/it, loss=53.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main file for training Yolo model on Pascal VOC and COCO dataset\n",
    "\"\"\"\n",
    "\n",
    "import config\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from utils import (\n",
    "    mean_average_precision,\n",
    "    cells_to_bboxes,\n",
    "    get_evaluation_bboxes,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint,\n",
    "    check_class_accuracy,\n",
    "    plot_couple_examples\n",
    ")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def train_fn(train_loader, model, optimizer, loss_fn, scaled_anchors): # scaler\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    #print(\"A> \", enumerate(loop), \" AHOJSTE\",loop,\" <B\")\n",
    "    losses = []\n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        #print(batch_idx,\" <x> \",x,\" <y> \",y, \" <>L\")\n",
    "        x = x.to(config.DEVICE)\n",
    "        y0, y1, y2 = (\n",
    "            y[0].to(config.DEVICE),\n",
    "            y[1].to(config.DEVICE),\n",
    "            y[2].to(config.DEVICE),\n",
    "        )\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(x)\n",
    "            loss = (\n",
    "                loss_fn(out[0], y0, scaled_anchors[0])\n",
    "                + loss_fn(out[1], y1, scaled_anchors[1])\n",
    "                + loss_fn(out[2], y2, scaled_anchors[2])\n",
    "            )\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        # scaler.scale(loss).backward()\n",
    "        # scaler.step(optimizer)\n",
    "        # scaler.update()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update progress bar\n",
    "        mean_loss = sum(losses) / len(losses)\n",
    "        loop.set_postfix(loss=mean_loss)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = YOLOv3(num_classes=config.NUM_CLASSES).to(config.DEVICE)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY\n",
    "    )\n",
    "    loss_fn = YoloLoss()\n",
    "    #scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # train_loader, test_loader, train_eval_loader = get_loaders(\n",
    "    #     train_csv_path=config.DATASET + \"/train.csv\", test_csv_path=config.DATASET + \"/train.csv\"\n",
    "    # )\n",
    "\n",
    "    train_dataset = YOLODataset(\n",
    "        config.DATASET+'/train.csv',\n",
    "        transform=config.train_transforms,\n",
    "        S=[config.IMAGE_SIZE // 32, config.IMAGE_SIZE // 16, config.IMAGE_SIZE // 8],\n",
    "        img_dir=config.IMG_DIR,\n",
    "        label_dir=config.LABEL_DIR,\n",
    "        anchors=config.ANCHORS,\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        num_workers=0,\n",
    "        pin_memory=config.PIN_MEMORY,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    test_dataset = YOLODataset(\n",
    "        config.DATASET+'/train.csv',\n",
    "        transform=config.train_transforms,\n",
    "        S=[config.IMAGE_SIZE // 32, config.IMAGE_SIZE // 16, config.IMAGE_SIZE // 8],\n",
    "        img_dir=config.IMG_DIR,\n",
    "        label_dir=config.LABEL_DIR,\n",
    "        anchors=config.ANCHORS,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        num_workers=0,\n",
    "        pin_memory=config.PIN_MEMORY,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # if config.LOAD_MODEL:\n",
    "    #     load_checkpoint(\n",
    "    #         config.CHECKPOINT_FILE, model, optimizer, config.LEARNING_RATE\n",
    "    #     )\n",
    "\n",
    "    scaled_anchors = (\n",
    "        torch.tensor(config.ANCHORS)\n",
    "        * torch.tensor(config.S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
    "    ).to(config.DEVICE)\n",
    "\n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        #plot_couple_examples(model, train_loader, 0.6, 0.5, scaled_anchors)\n",
    "        #train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors)\n",
    "        train_fn(train_loader, model, optimizer, loss_fn, scaled_anchors)\n",
    "\n",
    "        if config.SAVE_MODEL:\n",
    "            save_checkpoint(model, optimizer, filename=f\"checkpoint.pth.tar\")\n",
    "\n",
    "        #print(f\"Currently epoch {epoch}\")\n",
    "        #print(\"On Train Eval loader:\")\n",
    "        #check_class_accuracy(model, train_eval_loader, threshold=config.CONF_THRESHOLD)\n",
    "        #print(\"On Train loader:\")\n",
    "        #check_class_accuracy(model, train_loader, threshold=config.CONF_THRESHOLD)\n",
    "\n",
    "        if epoch % 10 == 0 and epoch > 0:\n",
    "            print(\"On Test loader:\")\n",
    "            check_class_accuracy(model, test_loader, threshold=config.CONF_THRESHOLD)\n",
    "\n",
    "            pred_boxes, true_boxes = get_evaluation_bboxes(\n",
    "                test_loader,\n",
    "                model,\n",
    "                iou_threshold=config.NMS_IOU_THRESH,\n",
    "                anchors=config.ANCHORS,\n",
    "                threshold=config.CONF_THRESHOLD,\n",
    "            )\n",
    "            mapval = mean_average_precision(\n",
    "                pred_boxes,\n",
    "                true_boxes,\n",
    "                iou_threshold=config.MAP_IOU_THRESH,\n",
    "                box_format=\"midpoint\",\n",
    "                num_classes=config.NUM_CLASSES,\n",
    "            )\n",
    "            print(f\"MAP: {mapval.item()}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
